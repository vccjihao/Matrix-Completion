/export/home/spark/spark-1.5.2/bin/spark-shell

lynx localhost:8080

/export/home/spark/spark-1.5.2/bin/spark-submit --class "SimpleApp" --master spark://192.168.5.1:7077 target/scala-2.10/simple-project_2.10-1.0.jar


from pyspark.mllib.linalg import Vectors
import numpy as np




import org.apache.spark._
import org.apache.spark.graphx._
import org.apache.spark.rdd.RDD

val users: RDD[(VertexId, (String, String))] = sc.parallelize(Array((3L, ("rxin", "student")), (7L, ("jgonzal", "postdoc")),(5L, ("franklin", "prof")), (2L, ("istoica", "prof"))))

val relationships: RDD[Edge[String]] = sc.parallelize(Array(Edge(3L, 7L, "collab"),    Edge(5L, 3L, "advisor"), Edge(2L, 5L, "colleague"), Edge(5L, 7L, "pi")))

val defaultUser = ("John Doe", "Missing")

val graph = Graph(users, relationships, defaultUser)

graph.vertices.filter { case (id, (name, pos)) => pos == "postdoc" }.count

graph.edges.filter(e => e.srcId > e.dstId).count

val facts: RDD[String] = graph.triplets.map(triplet =>triplet.srcAttr._1 + " is the " + triplet.attr + " of " + triplet.dstAttr._1)

facts.collect.foreach(println(_))
